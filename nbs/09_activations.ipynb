{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "import random,math,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "import fastcore.all as fc\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "\n",
    "from torch import tensor,nn,optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from datasets import load_dataset\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "mpl.rcParams['figure.constrained_layout.use'] = True\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)\n",
    "bs = 1024\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n",
    "\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs)\n",
    "dt = dls.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(ni, nf, ks=3, act=True):\n",
    "    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n",
    "    if act: res = nn.Sequential(res, nn.ReLU())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we use a 3Ã—3-pixel kernel. If we use eight filters then we will be using nine pixels to calculate eight numbers. That means it isn't really learning much at all. But if we use less filters, then there's not much being done in this first layer.\n",
    "\n",
    "To fix this, we can use a larger kernel in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_layers():\n",
    "    return [\n",
    "        conv(1 ,8, ks=5),        #14x14\n",
    "        conv(8 ,16),             #7x7\n",
    "        conv(16,32),             #4x4\n",
    "        conv(32,64),             #2x2\n",
    "        conv(64,10, act=False),  #1x1\n",
    "        nn.Flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train quickly, so that means training at a high learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [TrainCB(), CudaCB(), MetricsCB(Accuracy()), ProgressCB(plot=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, epochs=1):\n",
    "    learn = Learner(model, dls, loss_func=F.cross_entropy, lr=0.6, cbs=cbs)\n",
    "    learn.fit(epochs)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "learn = fit(nn.Sequential(*cnn_layers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.act_means = [[] for _ in layers]\n",
    "        self.act_stds  = [[] for _ in layers]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for i,l in enumerate(self.layers):\n",
    "            x = l(x)\n",
    "            self.act_means[i].append(x.data.cpu().mean())\n",
    "            self.act_stds [i].append(x.data.cpu().std ())\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self): return iter(self.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "model = SequentialModel(*cnn_layers())\n",
    "learn = fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.act_means: plt.plot(l)\n",
    "plt.legend(range(5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.act_stds: plt.plot(l)\n",
    "plt.legend(range(5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooks are PyTorch object you can add to any nn.Module. A hook will be called when a layer, it is registered to, is executed during the forward pass (forward hook) or the backward pass (backward hook). Hooks don't require us to rewrite the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "model = nn.Sequential(*cnn_layers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hook is attached to a layer, and needs to have a function that takes three arguments: module, input, output. Here we store the mean and std of the output in the correct position of our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_means = [[] for _ in model]\n",
    "act_stds  = [[] for _ in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(i, mod, inp, outp):\n",
    "    act_means[i].append(outp.data.cpu().mean())\n",
    "    act_stds [i].append(outp.data.cpu().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in enumerate(model): m.register_forward_hook(partial(append_stats, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in act_means: plt.plot(o)\n",
    "plt.legend(range(5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hook class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refactor this in a Hook class. It's very important to remove the hooks when they are deleted, otherwise there will be references kept and the memory won't be properly released when your model is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Hook():\n",
    "    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))\n",
    "    def remove(self): self.hook.remove()\n",
    "    def __del__(self): self.remove()\n",
    "\n",
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'stats'): hook.stats = ([],[])\n",
    "    acts = outp.data.cpu()\n",
    "    hook.stats[0].append(acts.mean())\n",
    "    hook.stats[1].append(acts.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "model = nn.Sequential(*cnn_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = [Hook(l, append_stats) for l in model[:5].children()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in hooks:\n",
    "    plt.plot(h.stats[0])\n",
    "    h.remove()\n",
    "plt.legend(range(5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Hooks class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Hooks(list):\n",
    "    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__ (self, *args): self.remove()\n",
    "    def __del__(self): self.remove()\n",
    "    def __delitem__(self, i):\n",
    "        self[i].remove()\n",
    "        super().__delitem__(i)\n",
    "    def remove(self):\n",
    "        for h in self: h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "model = nn.Sequential(*cnn_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Hooks(model, append_stats) as hooks:\n",
    "    fit(model)\n",
    "    fig,axs = plt.subplots(1,2, figsize=(10,4))\n",
    "    for h in hooks:\n",
    "        for i in 0,1: axs[i].plot(h.stats[i])\n",
    "    plt.legend(range(6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n",
    "    acts = outp.data.cpu()\n",
    "    hook.stats[0].append(acts.mean())\n",
    "    hook.stats[1].append(acts.std())\n",
    "    hook.stats[2].append(acts.histc(40,0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "model = nn.Sequential(*cnn_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Hooks(model, append_stats) as hooks: fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to @ste for initial version of histgram plotting code\n",
    "def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = subplots(2,2, figsize=(9,5))\n",
    "for ax,h in zip(axes.flatten(), hooks[:4]):\n",
    "    show_image(get_hist(h), ax, origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min(h):\n",
    "    h1 = torch.stack(h.stats[2]).t().float()\n",
    "    return h1[:2].sum(0)/h1.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = subplots(2,2, figsize=(15,6))\n",
    "for ax,h in zip(axes.flatten(), hooks[:4]):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
