{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8f8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.learner import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5eea66",
   "metadata": {},
   "source": [
    "## Setup Learner as in `07`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22868a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1024\n",
    "tds = dsd.with_transform(transformi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb11029",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dd(tds, bs)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82dcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,nh = 28*28,50\n",
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [TrainCB(), CudaCB(), MetricsCB(Accuracy()), ProgressCB(plot=True)]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d9e90c",
   "metadata": {},
   "source": [
    "## Mixed Precision â€“ Callback Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132af5c1",
   "metadata": {},
   "source": [
    "We'll use mixed-precision training as a more complex example to exercise our callback mechanism.\n",
    "\n",
    "If we take a look at the [previous implementation](https://github.com/fastai/fastai/blob/master/fastai/callback/fp16.py#L17) from `fastai 2`, we see that mixed precision requires injecting code at various stages during the training process. We'd like to do this as a callback, but training is a callback itself! We don't have enough places to hook on.\n",
    "\n",
    "More concretely, we'd need callback events for `after_pred`, `after_loss` and others. This is a simplified version of the code, without grad scaling.\n",
    "\n",
    "```Python\n",
    "class MixedPrecision(Callback):\n",
    "    order = 10\n",
    "    def before_fit(self): \n",
    "        self.autocast = autocast()\n",
    "    def before_batch(self): self.autocast.__enter__()\n",
    "    def after_pred(self):\n",
    "        if next(flatten(self.pred)).dtype==torch.float16: self.learn.pred = to_float(self.pred)\n",
    "    def after_loss(self): self.autocast.__exit__(None, None, None)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb14eb3",
   "metadata": {},
   "source": [
    "We have `before_fit` and `before_batch` callbacks in our `Learner` class, but `predict` and `get_loss` are part of `TrainCB`, and we don't have `before_` or `after_` hooks for them.\n",
    "\n",
    "What could we do?\n",
    "\n",
    "- Incorporate those functions inside the `Learner` class and create callbacks for them. This is the obvious and possibly easiest solution, but it kind of forces us to use inheritance as our main abstraction.\n",
    "- Subclass `TrainCB` creating a specialized version like `MixedPrecisionTrainCB`.\n",
    "\n",
    "We'll explore something else here, which is a way to **compose callbacks** without tightly coupling them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb6e26",
   "metadata": {},
   "source": [
    "Let's start by looking at the code we'd like to be able to see without worrying about how to do it:\n",
    "\n",
    "```Python\n",
    "cbs = [MixedPrecision(TrainCB()), MetricsCB(Accuracy()), ProgressCB()]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.001, cbs=cbs)\n",
    "learn.fit(1)\n",
    "```\n",
    "\n",
    "So `MixedPrecision` is a callback that wraps (or delegates to) `TrainCB`.\n",
    "\n",
    "(Side thought: this could potentially be a way to sort out callback ordering for tasks that need to occur in specific order)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2eccd6",
   "metadata": {},
   "source": [
    "To achieve this, we create a `CallbackWrapper` (not a very good name) that automatically creates callbacks for the public methods of a `wrapped` Callback object. It leverages the `with_cbs` decorator to create the new callbacks. There are some Python gymnastics involved, but the basic idea is just to create those `before_` and `after_` hooks without having to modify anything in the target callback. Perhaps there's a better way to achieve the same, but this helped me better understand Python decorators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d4d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import types\n",
    "\n",
    "class CallbackWrapper(Callback):\n",
    "    def __init__(self, wrapped):            \n",
    "        self.wrapped = wrapped\n",
    "        self.wrapped.callback = self.callback\n",
    "        for name, fn in inspect.getmembers(wrapped.__class__, predicate=inspect.isfunction):\n",
    "            if name.startswith(\"_\"): continue\n",
    "            # TODO: maybe ignore already wrapped?\n",
    "            # TODO: provide name mapping so `after_get_loss` -> `after_loss` if we want\n",
    "            setattr(wrapped, name, types.MethodType(with_cbs(name)(fn), wrapped))\n",
    "            \n",
    "    def __getattr__(self, name):\n",
    "        if name[0]=='_': raise AttributeError(name)\n",
    "        return getattr(self.wrapped, name)\n",
    "    \n",
    "    def __setattr__(self, name, value):\n",
    "        # Magic name\n",
    "        if name == \"wrapped\":\n",
    "            super().__setattr__(name, value)\n",
    "        else:\n",
    "            setattr(self.wrapped, name, value)\n",
    "            \n",
    "    def callback(self, method_nm):\n",
    "        getattr(self, method_nm, identity)()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba2827",
   "metadata": {},
   "source": [
    "With this in place we can now implement a simple tracing callback to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041804a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer(CallbackWrapper):\n",
    "    def after_predict(self): print(\"after_predict\")\n",
    "    def after_get_loss(self): print(\"after_get_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bda72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [Tracer(TrainCB()), CudaCB(), MetricsCB(Accuracy()), ProgressCB()]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.001, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9724f",
   "metadata": {},
   "source": [
    "And then we can implement a very basic mixed precision callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def flatten(lists): return chain.from_iterable(lists)\n",
    "\n",
    "# TODO: copy from fastai implementation to deal with lists, dicts, etc.\n",
    "def to_float(x):\n",
    "    return x.float() if torch.is_floating_point(x) else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecision(CallbackWrapper):\n",
    "    def before_fit(self):     self.autocast = torch.autocast(\"cuda\")\n",
    "    def before_batch(self):   self.autocast.__enter__()\n",
    "    def after_predict(self):  self.learn.preds = to_float(self.learn.preds)\n",
    "    def after_get_loss(self): self.autocast.__exit__(None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [MixedPrecision(TrainCB()), CudaCB(), MetricsCB(Accuracy()), ProgressCB(plot=True)]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62206eb",
   "metadata": {},
   "source": [
    "The method feels fragile. Should we do something simpler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6a718",
   "metadata": {},
   "source": [
    "## Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e9999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38473de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distributed(CallbackWrapper):\n",
    "    def __init__(self, wrapped):\n",
    "        super().__init__(wrapped)\n",
    "        self.accelerator = Accelerator()\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.learn.model, self.learn.opt, self.learn.dls = self.accelerator.prepare(\n",
    "            self.learn.model, self.learn.opt, self.learn.dls\n",
    "        )\n",
    "\n",
    "    # Replaces TrainCB.backward\n",
    "    def backward(self): self.accelerator.backward(self.learn.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [Distributed(TrainCB()), CudaCB(), MetricsCB(Accuracy()), ProgressCB(plot=True)]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e042d7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca534664",
   "metadata": {},
   "source": [
    "However, chaining multiple callbacks together doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671883fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [MixedPrecision(Distributed(TrainCB())), CudaCB(), MetricsCB(Accuracy()), ProgressCB(plot=True)]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ccf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [Distributed(MixedPrecision(TrainCB())), CudaCB(), MetricsCB(Accuracy()), ProgressCB(plot=True)]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e1ed6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
